
# ðŸš€ Kafka Streaming Pipeline with PySpark & PostgreSQL

This project implements a real-time, production-grade data streaming pipeline that simulates car telemetry data using a Kafka producer, performs schema validation and quality control using Spark Structured Streaming, and stores results into PostgreSQL.

---

## ðŸ“š Use Case

Millions of telemetry or log events are generated every second in real-world systems like ride-sharing, IoT, logistics, and vehicle monitoring. This project demonstrates a high-throughput ingestion and processing pipeline that ensures data correctness via schema and QC checks.

---

## ðŸ—ï¸ Architecture

```text
 Kafka Producer (Python) --> Kafka Topic (car_telemetry) --> Spark Structured Streaming
                                                             |
                                                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                                                 |                       |
                                         Clean Records               Corrupt Records
                                       â†’ PostgreSQL                  â†’ PostgreSQL
                                       (device_data)                 (corrupt_records)
```

---

## ðŸ“‚ Project Structure

```text
kafka-streaming-pipeline/
â”‚
â”œâ”€â”€ docker-compose.yaml         # Launches Kafka, Zookeeper, Postgres
â”œâ”€â”€ Dockerfile.spark            # Spark Docker image
â”œâ”€â”€ db_schema.sql               # SQL file to create Postgres tables
â”‚
â”œâ”€â”€ kafka_producer.py           # Kafka producer to simulate telemetry data
â”œâ”€â”€ logger_success.py           # Logs successful producer messages
â”œâ”€â”€ logger_failure.py           # Logs failed producer messages
â”‚
â”œâ”€â”€ producer_success.log        # Log of success messages
â”œâ”€â”€ producer_failure.log        # Log of failure messages
â”‚
â””â”€â”€ jobs/
    â”œâ”€â”€ consumer_grp.py         # PySpark consumer for schema & QC validation
    â”œâ”€â”€ spark_log.log           # Runtime logs
```

---

## ðŸ”§ Tech Stack

| Component         | Technology        |
|------------------|-------------------|
| Message Broker    | Kafka + Zookeeper |
| Stream Processor  | PySpark (Structured Streaming) |
| Storage           | PostgreSQL        |
| Containerization  | Docker + Compose  |
| Language          | Python 3.9+       |

---

## ðŸ“¦ PostgreSQL Table Design

### âœ… Clean Data Table: `device_data`

| Column               | Type        | Description                       |
|----------------------|-------------|-----------------------------------|
| trip_id              | STRING      | Unique trip identifier            |
| car_id               | STRING      | Car identifier                    |
| latitude             | DOUBLE      | Latitude at event time            |
| longitude            | DOUBLE      | Longitude at event time           |
| event_timestamp      | TIMESTAMP   | Original event timestamp          |
| speed_kmph           | DOUBLE      | Car speed                         |
| fuel_level           | DOUBLE      | Fuel level                        |
| engine_temp_c        | DOUBLE      | Engine temperature                |
| trip_start_time      | TIMESTAMP   | Trip start time                   |
| trip_start_latitude  | DOUBLE      | Latitude at trip start            |
| trip_start_longitude | DOUBLE      | Longitude at trip start           |
| trip_start_date      | DATE        | Partition key                     |
| message_key          | STRING      | Kafka message key                 |
| kafka_partition      | INT         | Kafka partition                   |
| kafka_offset         | BIGINT      | Kafka offset                      |
| load_timestamp       | TIMESTAMP   | Data ingestion time               |

---

### âŒ Corrupt Records Table: `corrupt_records`

| Column          | Type        | Description                       |
|------------------|-------------|-----------------------------------|
| message_key     | STRING      | Kafka message key                 |
| kafka_partition | INT         | Kafka partition                   |
| kafka_offset    | BIGINT      | Kafka offset                      |
| value_str       | STRING      | Raw JSON value                    |
| error_reason    | STRING      | Reason for failure                |
| load_timestamp  | TIMESTAMP   | Ingestion timestamp               |

---

### ðŸ“‹ Error Log Table: `consumer_error_log`

| Column         | Type        | Description                       |
|----------------|-------------|-----------------------------------|
| error_message  | STRING      | Full error traceback              |
| log_time       | TIMESTAMP   | Error occurrence time             |
| target_table   | STRING      | Table attempted to write          |
| batch_id       | STRING      | Spark batch ID                    |

---

## ðŸ› ï¸ Setup Instructions

### ðŸ” 1. Clone the Repo
```bash
git clone https://github.com/yourusername/kafka-streaming-pipeline.git
cd kafka-streaming-pipeline
```

### ðŸ³ 2. Start Kafka + Postgres + Spark
```bash
docker-compose up --build
```

### ðŸ—ƒï¸ 3. Create PostgreSQL Tables
```bash
psql -h localhost -U pst_docker -d kfk_sp_db -f db_schema.sql
```

### ðŸš— 4. Run Kafka Producer
```bash
python kafka_producer.py
```

### ðŸ”¥ 5. Run Spark Kafka Consumer
```bash
cd jobs
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.postgresql:postgresql:42.7.5 consumer_grp.py
```

---

## âœ… Features Completed

- [x] Kafka + Spark + PostgreSQL integration
- [x] Schema validation for incoming JSON
- [x] Raw & corrupt record separation
- [x] Column-wise QC logic in second layer (null checks, range validation)
- [x] Dockerized environment
- [ ] Streamlit Dashboard (coming soon)
- [ ] FastAPI producer for REST-based mock data (future)

---

## ðŸ“ˆ Example Use Cases

- IoT sensor pipelines  
- Real-time vehicle monitoring  
- Fraud detection pre-processing  
- Data lake staging before warehouse  

---

## ðŸ‘¨â€ðŸ’» Author

Developed by [Your Name].  
Want help integrating dashboards or alerts? Reach out or open a PR.

---

## ðŸ“„ License

This project is licensed under the MIT License.

'''
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0,org.postgresql:postgresql:42.7.5 --py-files con_grp_upd.py con_gr_upd.py > sc.log
'''